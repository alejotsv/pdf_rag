# ğŸ“š PDF RAG

PDF RAG is a lightweight Retrieval-Augmented Generation (RAG) chatbot designed to answer questions based on the content of **local PDF files**. It accepts **text questions** (image support coming soon), retrieves relevant content from the PDFs, and responds with a concise **answer** and **source reference**.

---

## ğŸš€ Features

- ğŸ“„ Load and index multiple local PDF files
- ğŸ” Semantic search using vector embeddings (via SpaCy + Chroma)
- ğŸ¤– Lightweight chatbot interface in the terminal
- ğŸ§  Question answering based on extracted PDF context
- ğŸ“š Source file and page number returned with each answer
- ğŸ§  Powered by a **local Ollama model** (e.g., `phi`, `mistral`)

---

## ğŸ› ï¸ Tech Stack

- **Python 3.12+**
- [`Ollama`](https://ollama.com) â€” local LLM backend (phi, mistral, gemma, etc.)
- [`ChromaDB`](https://www.trychroma.com) â€” vector store
- [`PyMuPDF`](https://pymupdf.readthedocs.io/) â€” PDF parsing
- [`SpaCy`](https://spacy.io/) â€” embeddings
- [`Tesseract`](https://github.com/tesseract-ocr/tesseract) â€” *(optional)* for screenshot text extraction (coming soon)

---

## âš™ï¸ Setup Instructions

### 1. Clone the Repository

```bash
git https://github.com/alejotsv/pdf_rag.git
cd pdf-rag
```

### 2. Create a Virtual Environment

```bash
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

---

## ğŸ§  Using a Local Model with Ollama

1. **Install Ollama**  
   [Ollama installation guide â†’](https://ollama.com/download)

2. **Pull a model**  
   (Choose one based on your system resources)

   ```bash
   ollama pull phi       # Good for low RAM (~2GB)
   ollama pull mistral   # Better reasoning, ~7â€“8GB RAM needed
   ```

3. **Run the model locally**

   In a separate terminal:

   ```bash
   ollama run phi
   ```

   You should see:
   ```
   >>>
   ```

4. **Set the model name in your `.env` file**

   Create a `.env` file at the root of your project:

   ```env
   OLLAMA_MODEL=phi
   OLLAMA_API_URL=http://localhost:11434
   ```

---

## â–¶ï¸ Running the Chatbot

In your main terminal (where your venv is activated):

```bash
python main.py
```

Then follow the prompt to paste your question. Press Enter twice to submit.

---

## ğŸ§ª Example Question

```
What is a B-tree index and when is it used?
```

---

## ğŸ“ Folder Structure

```
data/                # Your local PDFs go here
storage/             # Vector DB storage (auto-generated)
main.py              # Chat interface
embedder.py          # Embeds and stores chunks
docs_parser.py       # PDF text parsing and chunking
query.py             # Handles question answering
cleaning_rules.txt   # (optional) regex rules for cleaning footers, etc.
```

---

## ğŸ“Œ Notes

- If you run into memory errors, try using `phi` instead of `mistral`.
- The app currently supports **text input only** â€” screenshot OCR coming soon.
- To reprocess your PDFs, delete `storage/` and re-run the embed step.

---

## ğŸ§‘â€ğŸ’» Author

Made by [Alejandro Salas](https://github.com/alejotsv) â€” feel free to contribute or raise issues!

